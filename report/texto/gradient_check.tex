\section{Checagem de Gradiente}
O intuito deste algoritmo é realizar uma comparação entre os valores obtido pelo \textit{backpropagation} e a derivada numérica da função de custo $J$. Sua implementação foi bastante simples pois se tratava de um exercício conceitual, calculamos a derivada numérica pelo método da secante, utilizando um $\epsilon = 10^{-4}$ e, posteriormente, $\epsilon = 10^{-7}$. Em ambos os casos, obtivemos um erro médio (i.e. a diferença, em valor absoluto, entre cada gradiente do backpropagation o os valores obtidos pelo método da secante dividida pelo número total de neurônios da rede $N$) menor do que $\epsilon$ utilizado. Este método equivale a verificar que a norma 1 do vetor diferença entre os gradientes é inferior ao próprio $\epsilon \cdot N$; como há equivalência entre normas, é de se esperar que o resultado do algoritmo seja o mesmo, independente da escolha da norma.